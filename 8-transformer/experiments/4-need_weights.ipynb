{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output without weights: tensor([[ 0.0271,  0.0995, -0.1134, -0.2398,  0.0464, -0.2873,  0.0098, -0.2535,\n",
      "          0.0373, -0.0007,  0.1214, -0.0417, -0.0646, -0.1667, -0.0210, -0.0912],\n",
      "        [ 0.1833,  0.0517, -0.4616, -0.1873,  0.2027, -0.2449, -0.0279, -0.3226,\n",
      "          0.2955, -0.1358, -0.1031, -0.1092, -0.2429,  0.0221, -0.1686, -0.2252]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Attention output with weights: tensor([[ 0.0271,  0.0995, -0.1134, -0.2398,  0.0464, -0.2873,  0.0098, -0.2535,\n",
      "          0.0373, -0.0007,  0.1214, -0.0417, -0.0646, -0.1667, -0.0210, -0.0912],\n",
      "        [ 0.1833,  0.0517, -0.4616, -0.1873,  0.2027, -0.2449, -0.0279, -0.3226,\n",
      "          0.2955, -0.1358, -0.1031, -0.1092, -0.2429,  0.0221, -0.1686, -0.2252]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Attention weights without weights: None\n",
      "Attention weights with weights: tensor([[[0.1046, 0.1101, 0.0788, 0.1206, 0.0839, 0.1156, 0.0909, 0.1094,\n",
      "          0.1080, 0.0780],\n",
      "         [0.1026, 0.1240, 0.0885, 0.0901, 0.0760, 0.0836, 0.0692, 0.0964,\n",
      "          0.1405, 0.1292],\n",
      "         [0.1008, 0.1111, 0.1266, 0.0933, 0.0749, 0.1145, 0.0985, 0.1123,\n",
      "          0.0735, 0.0944],\n",
      "         [0.1465, 0.0550, 0.0887, 0.0582, 0.0863, 0.0562, 0.1053, 0.0729,\n",
      "          0.1188, 0.2120],\n",
      "         [0.1207, 0.1238, 0.1204, 0.1177, 0.0566, 0.1600, 0.0721, 0.1155,\n",
      "          0.0555, 0.0577],\n",
      "         [0.1944, 0.0957, 0.1120, 0.0705, 0.1075, 0.0899, 0.0938, 0.0822,\n",
      "          0.0561, 0.0979],\n",
      "         [0.0961, 0.1034, 0.0888, 0.1125, 0.1410, 0.0886, 0.0965, 0.0971,\n",
      "          0.0958, 0.0803],\n",
      "         [0.0826, 0.1076, 0.0743, 0.1192, 0.1099, 0.1253, 0.0822, 0.1109,\n",
      "          0.1170, 0.0711],\n",
      "         [0.1328, 0.0958, 0.1147, 0.0821, 0.1074, 0.1306, 0.0901, 0.0988,\n",
      "          0.0634, 0.0843],\n",
      "         [0.0559, 0.0725, 0.0502, 0.0974, 0.0948, 0.1361, 0.1153, 0.1250,\n",
      "          0.1272, 0.1257]],\n",
      "\n",
      "        [[0.0699, 0.0636, 0.1280, 0.1022, 0.0732, 0.1229, 0.1232, 0.0887,\n",
      "          0.1129, 0.1155],\n",
      "         [0.0996, 0.0916, 0.1052, 0.0954, 0.0868, 0.1411, 0.0791, 0.0971,\n",
      "          0.0934, 0.1108],\n",
      "         [0.0559, 0.1155, 0.1360, 0.0844, 0.0593, 0.1127, 0.1069, 0.1053,\n",
      "          0.1061, 0.1178],\n",
      "         [0.0727, 0.0384, 0.0921, 0.0824, 0.1837, 0.1091, 0.1454, 0.0467,\n",
      "          0.1388, 0.0908],\n",
      "         [0.0574, 0.1166, 0.1704, 0.0678, 0.0877, 0.1059, 0.0618, 0.1046,\n",
      "          0.1056, 0.1220],\n",
      "         [0.0899, 0.1335, 0.1086, 0.0781, 0.1092, 0.0713, 0.0380, 0.1198,\n",
      "          0.1403, 0.1114],\n",
      "         [0.1020, 0.1567, 0.0886, 0.0527, 0.0910, 0.0751, 0.1207, 0.0648,\n",
      "          0.1454, 0.1030],\n",
      "         [0.1206, 0.1196, 0.0425, 0.0552, 0.1329, 0.0629, 0.1015, 0.0752,\n",
      "          0.1419, 0.1476],\n",
      "         [0.0905, 0.0997, 0.0962, 0.1284, 0.0999, 0.0962, 0.1392, 0.1168,\n",
      "          0.0666, 0.0665],\n",
      "         [0.0983, 0.1380, 0.0805, 0.0972, 0.0831, 0.0923, 0.0917, 0.0945,\n",
      "          0.1117, 0.1127]]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "embed_dim = 16\n",
    "num_heads = 4\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create a MultiheadAttention layer\n",
    "mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "# Dummy inputs (shape: sequence length, batch size, embedding dimension)\n",
    "query = torch.randn(seq_len, batch_size, embed_dim)\n",
    "key = torch.randn(seq_len, batch_size, embed_dim)\n",
    "value = torch.randn(seq_len, batch_size, embed_dim)\n",
    "\n",
    "# Forward pass with need_weights set to False -- attention weights won't be computed\n",
    "attn_output_without, weights_without = mha(query, key, value, need_weights=False)\n",
    "attn_output_with, weights_with = mha(query, key, value, need_weights=True)\n",
    "\n",
    "print(\"Attention output without weights:\", attn_output_without[0])\n",
    "print(\"Attention output with weights:\", attn_output_with[0])\n",
    "print(\"Attention weights without weights:\", weights_without)\n",
    "print(\"Attention weights with weights:\", weights_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With need_weights=False:\n",
      "Output shape: torch.Size([10, 2, 16])\n",
      "Weights: None\n",
      "\n",
      "With need_weights=True:\n",
      "Output shape: torch.Size([10, 2, 16])\n",
      "Weights shape: torch.Size([2, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Call with need_weights=False. The second output will be None.\n",
    "output_without_weights, weights_without = mha(query, key, value, need_weights=False)\n",
    "print(\"With need_weights=False:\")\n",
    "print(\"Output shape:\", output_without_weights.shape)\n",
    "print(\"Weights:\", weights_without)\n",
    "\n",
    "# Call with need_weights=True. The second output will contain the attention weights.\n",
    "output_with_weights, weights_with = mha(query, key, value, need_weights=True)\n",
    "print(\"\\nWith need_weights=True:\")\n",
    "print(\"Output shape:\", output_with_weights.shape)\n",
    "print(\"Weights shape:\", weights_with.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
